{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 1111676,
          "sourceType": "datasetVersion",
          "datasetId": 623289
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Image_captioning_system",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityasingh345/image-caption-generator/blob/main/Image_captioning_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "adityajn105_flickr8k_path = kagglehub.dataset_download('adityajn105/flickr8k')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "OMdzQhBH58pV"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T11:48:06.564586Z",
          "iopub.execute_input": "2025-10-04T11:48:06.564758Z",
          "iopub.status.idle": "2025-10-04T11:48:08.295233Z",
          "shell.execute_reply.started": "2025-10-04T11:48:06.564741Z",
          "shell.execute_reply": "2025-10-04T11:48:08.294429Z"
        },
        "id": "ER8w6QE258pW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator , load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# sequneces is used to create a custom data generator for training batches\n",
        "from tensorflow.keras.utils import Sequence\n",
        "# to_categorical is used for one-hot  encoding categorical labels\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "# Keras layers for building different parts of the neural network, like convolutions, dense layers, and activation functions\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer\n",
        "# Additional layers for embedding, LSTM (for sequence modeling), and layer combinations\n",
        "from tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional\n",
        "\n",
        "# Pre-trained models (VGG16, ResNet50, DenseNet201) from Keras applications to use as feature extractors\n",
        "from tensorflow.keras.applications import VGG16, ResNet50, DenseNet201\n",
        "\n",
        "# Adam optimizer for training models, widely used for its adaptive learning rate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Callbacks to monitor training, such as saving the best model, early stopping, and reducing learning rate on plateau\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Warnings module to filter out unwanted warnings during execution\n",
        "import warnings\n",
        "\n",
        "# Matplotlib for creating visualizations (e.g., loss curves, model performance)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Seaborn for statistical data visualization, often used to create better-looking plots\n",
        "import seaborn as sns\n",
        "\n",
        "# textwrap for wrapping long text into shorter lines, useful for displaying captions or summaries\n",
        "from textwrap import wrap\n",
        "\n",
        "# Configuring the plotting style and font sizes for better readability of graphs\n",
        "\n",
        "plt.rcParams['font.size'] = 12  # Set the font size for matplotlib plots\n",
        "sns.set_style(\"dark\")  # Set a dark style for seaborn plots\n",
        "warnings.filterwarnings('ignore')  # Ignore warnings for cleaner output\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T11:48:08.295969Z",
          "iopub.execute_input": "2025-10-04T11:48:08.29629Z",
          "iopub.status.idle": "2025-10-04T11:48:21.623646Z",
          "shell.execute_reply.started": "2025-10-04T11:48:08.296273Z",
          "shell.execute_reply": "2025-10-04T11:48:21.622774Z"
        },
        "id": "Kz2t-l1i58pX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/kaggle/input/flickr8k/Images'"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T11:48:21.625527Z",
          "iopub.execute_input": "2025-10-04T11:48:21.62614Z",
          "iopub.status.idle": "2025-10-04T11:48:21.62942Z",
          "shell.execute_reply.started": "2025-10-04T11:48:21.626118Z",
          "shell.execute_reply": "2025-10-04T11:48:21.628657Z"
        },
        "id": "utxFaJTU58pY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/kaggle/input/flickr8k/captions.txt')\n",
        "data.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T11:48:21.63015Z",
          "iopub.execute_input": "2025-10-04T11:48:21.630462Z",
          "iopub.status.idle": "2025-10-04T11:48:21.755131Z",
          "shell.execute_reply.started": "2025-10-04T11:48:21.630435Z",
          "shell.execute_reply": "2025-10-04T11:48:21.754354Z"
        },
        "id": "4vGVo2pt58pY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def readImg(path, img_size = 224):\n",
        "    img = load_img(path, color_mode='rgb', target_size = (img_size, img_size))\n",
        "    img = img_to_array(img)\n",
        "    img = img / 255.\n",
        "\n",
        "    return img\n",
        "\n",
        "def display_images(df):\n",
        "    df = df.reset_index(drop = True)\n",
        "    plt.figure(figsize = (20 ,20))\n",
        "    n = 0\n",
        "    for i in range(15):\n",
        "        n+= 1\n",
        "        plt.subplot(5, 5, n)\n",
        "        plt.subplots_adjust(hspace = 0.7 , wspace = 0.3)\n",
        "\n",
        "\n",
        "        image = readImg(f\"/kaggle/input/flickr8k/Images/{df.image[i]}\")\n",
        "        plt.imshow(image)\n",
        "        plt.title(\"\\n\".join(wrap(df.caption[i], 20)))\n",
        "\n",
        "        plt.axis(\"off\")\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T11:48:21.756017Z",
          "iopub.execute_input": "2025-10-04T11:48:21.756282Z",
          "iopub.status.idle": "2025-10-04T11:48:21.762683Z",
          "shell.execute_reply.started": "2025-10-04T11:48:21.756259Z",
          "shell.execute_reply": "2025-10-04T11:48:21.761718Z"
        },
        "id": "WiAnwmO558pZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "display_images(data.sample(15))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T11:48:21.76351Z",
          "iopub.execute_input": "2025-10-04T11:48:21.763693Z",
          "iopub.status.idle": "2025-10-04T11:48:24.334859Z",
          "shell.execute_reply.started": "2025-10-04T11:48:21.763677Z",
          "shell.execute_reply": "2025-10-04T11:48:24.333965Z"
        },
        "id": "gTSpZqmL58pZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# now comes the part of text pre processing\n",
        "def text_preprocessing(data):\n",
        "    # In Python, lambda is a way to create a small anonymous function\n",
        "    #(a function without a name) in a single line. It's often used when\n",
        "    #you need a simple function for a short task, like in apply().\n",
        "    data['caption'] = data['caption'].apply(lambda x: x.lower())\n",
        "    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\", \"\"))\n",
        "    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\", \" \"))\n",
        "    data['caption'] = data['caption'].apply(lambda x: x.replace(\"startseq\", \"\").replace(\"endseq\", \"\").strip())\n",
        "    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n",
        "    data['caption'] = \"startseq \" + data['caption']+\" endseq\"\n",
        "    return data"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T11:48:24.335715Z",
          "iopub.execute_input": "2025-10-04T11:48:24.335939Z",
          "iopub.status.idle": "2025-10-04T11:48:24.341621Z",
          "shell.execute_reply.started": "2025-10-04T11:48:24.335919Z",
          "shell.execute_reply": "2025-10-04T11:48:24.340735Z"
        },
        "id": "Z7duACIw58pZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data = text_preprocessing(data)\n",
        "captions = data['caption'].tolist()\n",
        "captions[:10]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T11:48:24.342581Z",
          "iopub.execute_input": "2025-10-04T11:48:24.34281Z",
          "iopub.status.idle": "2025-10-04T11:48:24.480277Z",
          "shell.execute_reply.started": "2025-10-04T11:48:24.34279Z",
          "shell.execute_reply": "2025-10-04T11:48:24.479599Z"
        },
        "id": "eooCB3i758pZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# now we will tokenize\n",
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T11:48:24.48233Z",
          "iopub.execute_input": "2025-10-04T11:48:24.482618Z",
          "iopub.status.idle": "2025-10-04T11:48:24.48622Z",
          "shell.execute_reply.started": "2025-10-04T11:48:24.482598Z",
          "shell.execute_reply": "2025-10-04T11:48:24.485584Z"
        },
        "id": "aI0Up9u258pa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length = max(len(caption.split()) for caption in captions)\n",
        "\n",
        "images = data['image'].unique().tolist()\n",
        "nimages = len(images)\n",
        "\n",
        "split_index = round(0.85*nimages)\n",
        "train_images = images[:split_index]\n",
        "val_images = images[split_index:]\n",
        "\n",
        "train = data[data['image'].isin(train_images)]\n",
        "test = data[data['image'].isin(val_images)]\n",
        "\n",
        "train.reset_index(inplace = True, drop = True)\n",
        "test.reset_index(inplace = True, drop=True)\n",
        "\n",
        "tokenizer.texts_to_sequences(captions[1])[0]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T11:48:24.486862Z",
          "iopub.execute_input": "2025-10-04T11:48:24.487114Z",
          "iopub.status.idle": "2025-10-04T11:48:24.971869Z",
          "shell.execute_reply.started": "2025-10-04T11:48:24.48709Z",
          "shell.execute_reply": "2025-10-04T11:48:24.971083Z"
        },
        "id": "ZE1iUM7j58pa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# pre trained model for fetur extracting\n",
        "model = DenseNet201()\n",
        "# initalizing the model\n",
        "\n",
        "fe = Model(inputs = model.input , outputs = model.layers[-2].output)\n",
        "\n",
        "img_size = 224\n",
        "\n",
        "features = {}\n",
        "\n",
        "# looping through each image\n",
        "for image in tqdm(data['image'].unique().tolist()):\n",
        "    img = load_img(os.path.join(image_path , image), target_size = (img_size, img_size))\n",
        "    img = img_to_array(img)\n",
        "\n",
        "    img = img / 255\n",
        "\n",
        "    img = np.expand_dims(img, axis = 0)\n",
        "\n",
        "    feature = fe.predict(img , verbose = 0)\n",
        "\n",
        "    features[image] = feature"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T12:09:44.381573Z",
          "iopub.execute_input": "2025-10-04T12:09:44.382379Z",
          "iopub.status.idle": "2025-10-04T12:24:35.341683Z",
          "shell.execute_reply.started": "2025-10-04T12:09:44.382352Z",
          "shell.execute_reply": "2025-10-04T12:24:35.341034Z"
        },
        "id": "DnYeJWdf58pa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "class CustomDataGenerator(Sequence):\n",
        "    # \"\"\"\n",
        "    # Custom Data Generator to yield batches of image features and corresponding caption sequences.\n",
        "\n",
        "    # Attributes:\n",
        "    # -----------\n",
        "    # df : DataFrame\n",
        "    #     DataFrame containing image identifiers and their corresponding captions.\n",
        "    # X_col : str\n",
        "    #     Name of the column in the DataFrame that contains image identifiers.\n",
        "    # y_col : str\n",
        "    #     Name of the column in the DataFrame that contains captions.\n",
        "    # batch_size : int\n",
        "    #     Number of samples per batch.\n",
        "    # directory : str\n",
        "    #     Directory where the images are stored.\n",
        "    # tokenizer : Tokenizer\n",
        "    #     Tokenizer object used to convert text into sequences of integers.\n",
        "    # vocab_size : int\n",
        "    #     Size of the vocabulary for the output categorical sequences.\n",
        "    # max_length : int\n",
        "    #     Maximum length for the input sequences (captions) to be padded.\n",
        "    # features : dict\n",
        "    #     Dictionary containing image features extracted from a pre-trained model.\n",
        "    # shuffle : bool, optional\n",
        "    #     Whether to shuffle the data at the end of each epoch (default is True).\n",
        "    # n : int\n",
        "    #     Total number of samples in the DataFrame.\n",
        "    # \"\"\"\n",
        "\n",
        "    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer,\n",
        "                 vocab_size, max_length, features, shuffle=True):\n",
        "        \"\"\"Initializes the generator with the given parameters.\"\"\"\n",
        "        self.df = df.copy()  # Make a copy of the DataFrame to avoid modifying the original one\n",
        "        self.X_col = X_col  # Column name for image identifiers\n",
        "        self.y_col = y_col  # Column name for captions\n",
        "        self.directory = directory  # Directory where images are stored\n",
        "        self.batch_size = batch_size  # Number of samples in each batch\n",
        "        self.tokenizer = tokenizer  # Tokenizer to convert text to sequences\n",
        "        self.vocab_size = vocab_size  # Size of the vocabulary for the captions\n",
        "        self.max_length = max_length  # Maximum length of input sequences for padding\n",
        "        self.features = features  # Pre-extracted features of the images\n",
        "        self.shuffle = shuffle  # Whether to shuffle the data at the end of each epoch\n",
        "        self.n = len(self.df)  # Total number of samples\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Shuffles the DataFrame at the end of each epoch if shuffle is True.\"\"\"\n",
        "        if self.shuffle:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)  # Shuffle the DataFrame\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of batches per epoch.\"\"\"\n",
        "        return self.n // self.batch_size  # Floor division to get the number of complete batches\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generates one batch of data.\"\"\"\n",
        "        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size, :]  # Get batch samples\n",
        "        X1, X2, y = self.__get_data(batch)  # Get data for the batch\n",
        "        return (X1, X2), y  # Return inputs and output as a tuple\n",
        "\n",
        "    def __get_data(self, batch):\n",
        "        \"\"\"Generates data for a given batch of samples.\"\"\"\n",
        "        X1, X2, y = list(), list(), list()  # Initialize empty lists to store features, input sequences, and outputs\n",
        "\n",
        "        images = batch[self.X_col].tolist()  # Get list of image identifiers in the batch\n",
        "\n",
        "        for image in images:\n",
        "            feature = self.features[image][0]  # Extract pre-computed image feature from the features dictionary\n",
        "            captions = batch.loc[batch[self.X_col] == image, self.y_col].tolist()  # Get captions for the image\n",
        "            for caption in captions:\n",
        "                seq = self.tokenizer.texts_to_sequences([caption])[0]  # Convert caption to sequence of integers\n",
        "\n",
        "                for i in range(1, len(seq)):\n",
        "                    # Split the caption into input and output sequences\n",
        "                    in_seq, out_seq = seq[:i], seq[i]  # Input sequence and the next word as the output\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]  # Pad input sequence to max_length\n",
        "                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]  # Convert output to one-hot encoding\n",
        "\n",
        "                    X1.append(feature)  # Append image feature as input\n",
        "                    X2.append(in_seq)  # Append the input sequence\n",
        "                    y.append(out_seq)  # Append the output (next word in sequence)\n",
        "\n",
        "        # Convert lists to NumPy arrays for better performance\n",
        "        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
        "\n",
        "        return X1, X2, y  # Return image features, input sequences, and output sequences"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T12:25:11.463518Z",
          "iopub.execute_input": "2025-10-04T12:25:11.4641Z",
          "iopub.status.idle": "2025-10-04T12:25:11.475029Z",
          "shell.execute_reply.started": "2025-10-04T12:25:11.464076Z",
          "shell.execute_reply": "2025-10-04T12:25:11.474404Z"
        },
        "id": "6cQaWzgy58pb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Reshape, Embedding, LSTM, Dropout, add, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define two input layers\n",
        "input1 = Input(shape=(1920,))  # Input for image features\n",
        "# input1 = [0.23, 0.54, ..., 0.12]\n",
        "input2 = Input(shape=(max_length,))  # Input for the caption sequence\n",
        "# input2 = np.array([[1 , 5 , 7, 0, ]]) shape( 1 , 5) example 'start dog runs'\n",
        "\n",
        "# Image feature processing\n",
        "img_features = Dense(256, activation='relu')(input1)  # Fully connected layer to reduce dimensionality\n",
        "img_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)  # Reshape to (1, 256) to concatenate with LSTM output\n",
        "# reshape converts (1, 256) into (1,1,256) so it can be concatenated with captions\n",
        "\n",
        "\n",
        "# Caption (text) processing\n",
        "sentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n",
        " # shape(1 , 5 , 256) 1 -> batch size 1-> max_length 256 -> embedding_dim # Embedding layer for input captions\n",
        "merged = concatenate([img_features_reshaped, sentence_features], axis=1)  # Concatenate image features with caption sequence\n",
        "sentence_features = LSTM(256)(merged)  # LSTM processes the combined features\n",
        "\n",
        "# Combine LSTM output with image features\n",
        "x = Dropout(0.5)(sentence_features)  # Dropout to prevent overfitting\n",
        "x = add([x, img_features])  # Skip connection to add image features to LSTM output\n",
        "x = Dense(128, activation='relu')(x)  # Fully connected layer\n",
        "x = Dropout(0.5)(x)  # Dropout for regularization\n",
        "output = Dense(vocab_size, activation='softmax')(x)  # Output layer with softmax activation to predict the next word\n",
        "\n",
        "# Create and compile the model\n",
        "caption_model = Model(inputs=[input1, input2], outputs=output)  # Define the model with two inputs and one output\n",
        "caption_model.compile(loss='categorical_crossentropy', optimizer='adam')  # Compile the model with categorical crossentropy loss\n",
        "\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(caption_model)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T12:25:13.432145Z",
          "iopub.execute_input": "2025-10-04T12:25:13.432859Z",
          "iopub.status.idle": "2025-10-04T12:25:13.619001Z",
          "shell.execute_reply.started": "2025-10-04T12:25:13.432838Z",
          "shell.execute_reply": "2025-10-04T12:25:13.618392Z"
        },
        "id": "_9iKpN6l58pb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "caption_model.summary()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T12:25:18.041831Z",
          "iopub.execute_input": "2025-10-04T12:25:18.042575Z",
          "iopub.status.idle": "2025-10-04T12:25:18.064035Z",
          "shell.execute_reply.started": "2025-10-04T12:25:18.042548Z",
          "shell.execute_reply": "2025-10-04T12:25:18.063345Z"
        },
        "id": "wIvTmkNF58pb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a training data generator\n",
        "train_generator = CustomDataGenerator(\n",
        "    df=train,  # DataFrame containing training data\n",
        "    X_col='image',  # Column name with image identifiers\n",
        "    y_col='caption',  # Column name with captions\n",
        "    batch_size=64,  # Number of samples per batch\n",
        "    directory=image_path,  # Path to the directory containing images\n",
        "    tokenizer=tokenizer,  # Tokenizer to convert captions to sequences\n",
        "    vocab_size=vocab_size,  # Total vocabulary size for one-hot encoding\n",
        "    max_length=max_length,  # Maximum length for input sequences\n",
        "    features=features  # Pre-computed image features\n",
        ")\n",
        "\n",
        "# Create a validation data generator\n",
        "validation_generator = CustomDataGenerator(\n",
        "    df=test,  # DataFrame containing validation data\n",
        "    X_col='image',  # Column name with image identifiers\n",
        "    y_col='caption',  # Column name with captions\n",
        "    batch_size=64,  # Number of samples per batch\n",
        "    directory=image_path,  # Path to the directory containing images\n",
        "    tokenizer=tokenizer,  # Tokenizer to convert captions to sequences\n",
        "    vocab_size=vocab_size,  # Total vocabulary size for one-hot encoding\n",
        "    max_length=max_length,  # Maximum length for input sequences\n",
        "    features=features  # Pre-computed image features\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T12:25:18.373344Z",
          "iopub.execute_input": "2025-10-04T12:25:18.37362Z",
          "iopub.status.idle": "2025-10-04T12:25:18.382209Z",
          "shell.execute_reply.started": "2025-10-04T12:25:18.373599Z",
          "shell.execute_reply": "2025-10-04T12:25:18.381438Z"
        },
        "id": "slkA31EZ58pc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Callback to save the model with the lowest validation loss\n",
        "model_name = \"model.keras\"  # Change the file extension to '.keras'\n",
        "checkpoint = ModelCheckpoint(\n",
        "    model_name,  # Filepath where the model will be saved\n",
        "    monitor=\"val_loss\",  # Monitor the validation loss during training\n",
        "    mode=\"min\",  # Save the model when the validation loss is minimized\n",
        "    save_best_only=True,  # Save only the model with the best validation loss\n",
        "    verbose=1  # Print a message when the model is saved\n",
        ")\n",
        "\n",
        "# Callback to stop training early if the validation loss does not improve\n",
        "earlystopping = EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor the validation loss\n",
        "    min_delta=0,  # Minimum change in the monitored value to be considered an improvement\n",
        "    patience=5,  # Number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,  # Print a message when early stopping is triggered\n",
        "    restore_best_weights=True  # Restore model weights from the epoch with the best validation loss\n",
        ")\n",
        "\n",
        "# Callback to reduce the learning rate if validation loss does not improve\n",
        "learning_rate_reduction = ReduceLROnPlateau(\n",
        "    monitor='val_loss',  # Monitor the validation loss\n",
        "    patience=3,  # Number of epochs with no improvement after which the learning rate will be reduced\n",
        "    verbose=1,  # Print a message when the learning rate is reduced\n",
        "    factor=0.2,  # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n",
        "    min_lr=1e-8  # Lower bound on the learning rate to avoid reducing it too much\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T12:25:18.729658Z",
          "iopub.execute_input": "2025-10-04T12:25:18.729897Z",
          "iopub.status.idle": "2025-10-04T12:25:18.735243Z",
          "shell.execute_reply.started": "2025-10-04T12:25:18.729879Z",
          "shell.execute_reply": "2025-10-04T12:25:18.734539Z"
        },
        "id": "mD2-CGgd58pc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the image captioning model\n",
        "history = caption_model.fit(\n",
        "    train_generator,  # Training data generator\n",
        "    epochs=50,  # Maximum number of epochs to train the model\n",
        "    validation_data=validation_generator,  # Validation data generator\n",
        "    callbacks=[checkpoint, earlystopping, learning_rate_reduction]  # List of callbacks to monitor and control training\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T12:25:20.319366Z",
          "iopub.execute_input": "2025-10-04T12:25:20.319836Z",
          "iopub.status.idle": "2025-10-04T12:42:55.099789Z",
          "shell.execute_reply.started": "2025-10-04T12:25:20.31981Z",
          "shell.execute_reply": "2025-10-04T12:42:55.099145Z"
        },
        "id": "_KqX1Q6l58pc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def idx_to_word(integer, tokenizer):\n",
        "    \"\"\"\n",
        "    Converts an integer (word index) back to its corresponding word\n",
        "    using the tokenizer's word index.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    integer : int\n",
        "        The index of the word to be converted.\n",
        "    tokenizer : Tokenizer\n",
        "        The tokenizer that contains the mapping of words to indices.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str or None\n",
        "        The word corresponding to the given index, or None if the index is not found.\n",
        "    \"\"\"\n",
        "    for word, index in tokenizer.word_index.items():  # Loop through all word-index pairs in the tokenizer\n",
        "        if index == integer:  # If the index matches the given integer\n",
        "            return word  # Return the corresponding word\n",
        "    return None  # If no match is found, return None"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T12:43:52.772582Z",
          "iopub.execute_input": "2025-10-04T12:43:52.772853Z",
          "iopub.status.idle": "2025-10-04T12:43:52.777306Z",
          "shell.execute_reply.started": "2025-10-04T12:43:52.772833Z",
          "shell.execute_reply": "2025-10-04T12:43:52.776616Z"
        },
        "id": "ivtdtdc558pd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_caption(model, image, tokenizer, max_length, features):\n",
        "    \"\"\"\n",
        "    Generates a caption for a given image using the trained captioning model.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : keras.Model\n",
        "        The trained image captioning model.\n",
        "    image : str\n",
        "        The identifier or key for the image in the features dictionary.\n",
        "    tokenizer : Tokenizer\n",
        "        The tokenizer object used to convert text into sequences of integers.\n",
        "    max_length : int\n",
        "        The maximum allowed length for the caption.\n",
        "    features : dict\n",
        "        A dictionary containing pre-extracted image features for all images.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        The generated caption for the image.\n",
        "    \"\"\"\n",
        "\n",
        "    feature = features[image]  # Extract the feature for the given image\n",
        "    in_text = \"startseq\"  # Start the caption generation with the starting token\n",
        "\n",
        "    for i in range(max_length):  # Limit the length of the caption to max_length\n",
        "        # Convert the input text so far into a sequence of integers\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # Pad the sequence to ensure it has the required max_length\n",
        "        sequence = pad_sequences([sequence], max_length)\n",
        "\n",
        "        # Predict the next word's index using the image feature and input sequence\n",
        "        y_pred = model.predict([feature, sequence], verbose=0)  # Model predicts probabilities for each word in the vocabulary\n",
        "        y_pred = np.argmax(y_pred)  # Choose the word with the highest probability\n",
        "\n",
        "        word = idx_to_word(y_pred, tokenizer)  # Convert the predicted index back to a word\n",
        "\n",
        "        if word is None:  # If no matching word is found, stop generation\n",
        "            break\n",
        "\n",
        "        in_text += \" \" + word  # Add the predicted word to the caption\n",
        "\n",
        "        if word == 'endseq':  # If the 'endseq' token is predicted, end the generation\n",
        "            break\n",
        "\n",
        "    return in_text  # Return the full caption as a string"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T12:44:04.84579Z",
          "iopub.execute_input": "2025-10-04T12:44:04.846114Z",
          "iopub.status.idle": "2025-10-04T12:44:04.851784Z",
          "shell.execute_reply.started": "2025-10-04T12:44:04.84609Z",
          "shell.execute_reply": "2025-10-04T12:44:04.851174Z"
        },
        "id": "HEjtRVrZ58pd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "samples = test.sample(20)\n",
        "samples.reset_index(drop=True,inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T12:44:11.04003Z",
          "iopub.execute_input": "2025-10-04T12:44:11.040566Z",
          "iopub.status.idle": "2025-10-04T12:44:11.044996Z",
          "shell.execute_reply.started": "2025-10-04T12:44:11.040544Z",
          "shell.execute_reply": "2025-10-04T12:44:11.044405Z"
        },
        "id": "zwVEvfVE58pd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for index,record in samples.iterrows():\n",
        "\n",
        "    img = load_img(os.path.join(image_path,record['image']),target_size=(224,224))\n",
        "    img = img_to_array(img)\n",
        "    img = img/255.\n",
        "\n",
        "    caption = predict_caption(caption_model, record['image'], tokenizer, max_length, features)\n",
        "    samples.loc[index,'caption'] = caption"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T12:44:16.180247Z",
          "iopub.execute_input": "2025-10-04T12:44:16.181098Z",
          "iopub.status.idle": "2025-10-04T12:44:32.985115Z",
          "shell.execute_reply.started": "2025-10-04T12:44:16.181049Z",
          "shell.execute_reply": "2025-10-04T12:44:32.984305Z"
        },
        "id": "BOF9ez_E58pd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "display_images(samples)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T12:44:35.370956Z",
          "iopub.execute_input": "2025-10-04T12:44:35.37166Z",
          "iopub.status.idle": "2025-10-04T12:44:37.695007Z",
          "shell.execute_reply.started": "2025-10-04T12:44:35.371634Z",
          "shell.execute_reply": "2025-10-04T12:44:37.693994Z"
        },
        "id": "lE6xlv4d58pe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the image filename you want to test\n",
        "image_name = '1024138940_f1fefbdce1.jpg'  # replace with your image name\n",
        "\n",
        "# Check if the image exists in the DataFrame\n",
        "matching_records = samples[samples['image'] == image_name]\n",
        "\n",
        "if not matching_records.empty:\n",
        "    # If a matching record exists, access it\n",
        "    record = matching_records.iloc[0]\n",
        "    print(f\"Record found: {record}\")\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    img = load_img(os.path.join(image_path, record['image']), target_size=(224, 224))\n",
        "    img = img_to_array(img)\n",
        "    img = img / 255.0  # Normalize\n",
        "\n",
        "    # Generate the caption using the caption model\n",
        "    caption = predict_caption(caption_model, record['image'], tokenizer, max_length, features)\n",
        "\n",
        "    # Print the predicted caption\n",
        "    print(f\"Predicted Caption for {image_name}: {caption}\")\n",
        "\n",
        "    # If you want to visualize the image:\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Load and display the image\n",
        "    img = load_img(os.path.join(image_path, record['image']))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Predicted Caption: {caption}\")\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(f\"No matching record found for {image_name}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T13:00:12.176409Z",
          "iopub.execute_input": "2025-10-04T13:00:12.176903Z",
          "iopub.status.idle": "2025-10-04T13:00:12.184084Z",
          "shell.execute_reply.started": "2025-10-04T13:00:12.17688Z",
          "shell.execute_reply": "2025-10-04T13:00:12.183356Z"
        },
        "id": "3rg6zkmp58pe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Function to display image and caption\n",
        "def display_image_and_caption(image_path, image_name, caption):\n",
        "    # Load the image\n",
        "    img = mpimg.imread(os.path.join(image_path, image_name))\n",
        "\n",
        "    # Plot the image and display the caption\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')  # Turn off the axis\n",
        "    plt.title(caption, fontsize=14)\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T12:45:07.375133Z",
          "iopub.execute_input": "2025-10-04T12:45:07.375838Z",
          "iopub.status.idle": "2025-10-04T12:45:07.380317Z",
          "shell.execute_reply.started": "2025-10-04T12:45:07.375813Z",
          "shell.execute_reply": "2025-10-04T12:45:07.379533Z"
        },
        "id": "BEsj8nVL58pe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "image_name = '1024138940_f1fefbdce1.jpg'  # Example image name\n",
        "caption = predict_caption(caption_model, image_name, tokenizer, max_length, features)\n",
        "\n",
        "# Display the image along with the generated caption\n",
        "display_image_and_caption(image_path, image_name, caption)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T12:45:14.552547Z",
          "iopub.execute_input": "2025-10-04T12:45:14.552823Z",
          "iopub.status.idle": "2025-10-04T12:45:15.526131Z",
          "shell.execute_reply.started": "2025-10-04T12:45:14.552803Z",
          "shell.execute_reply": "2025-10-04T12:45:15.525462Z"
        },
        "id": "pVKYnyqY58pe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "image_name = '106490881_5a2dd9b7bd.jpg'  # Example image name\n",
        "caption = predict_caption(caption_model, image_name, tokenizer, max_length, features)\n",
        "\n",
        "# Display the image along with the generated caption\n",
        "display_image_and_caption(image_path, image_name, caption)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-04T12:47:02.995735Z",
          "iopub.execute_input": "2025-10-04T12:47:02.995991Z",
          "iopub.status.idle": "2025-10-04T12:47:04.10559Z",
          "shell.execute_reply.started": "2025-10-04T12:47:02.995964Z",
          "shell.execute_reply": "2025-10-04T12:47:04.1049Z"
        },
        "id": "UjjvjOpl58pf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "lYP7OUG758pf"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}